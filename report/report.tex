\documentclass[twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[style=numeric]{biblatex}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{microtype}

% Bibliography file
\addbibresource{references.bib}

\title{Quantifind: Finding Optimal Quantization Strategies For Diverse Neural Networks}
\author{Varun Ramani}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Your abstract goes here.
\end{abstract}

\section{Introduction}
As deep neural networks continue to dominate the landscape of machine learning
applications, the design of domain-specific hardware accelerators has become
increasingly critical for efficient deployment in deep learning situations.
While these specialized architectures offer significant performance improvements
over general-purpose processors, their design space is heavily influenced by
fundamental assumptions about numerical precision and data representation. One
such assumption -- the bit width used for neural network operations -- can
dramatically impact both the hardware complexity and the achievable performance
of these accelerators.

Model quantization, the process of reducing numerical precision of network
parameters and computations, represents a crucial bridge between neural network
algorithms and their hardware implementations. While current domain-specific
architectures often implement fixed bit widths (typically 8-bit integers or
16-bit floating point numbers) based on conventional wisdom or hardware
constraints, there has been limited systematic investigation into how different
neural architectures respond to varying levels of quantization precision. This
gap in understanding affects not only software optimization but also hardware
design decisions, as the choice of supported bit widths directly impacts circuit
complexity, memory requirements, and energy efficiency of neural network
accelerators.

The motivation for quantization stems from both practical and theoretical
considerations in neural network deployment. From a hardware perspective,
reduced precision operations offer substantial benefits: lower memory bandwidth
requirements, decreased power consumption, and simpler arithmetic units that can
be parallelized more effectively. For example, replacing 32-bit floating-point
operations with 8-bit integer operations can theoretically yield a 4x reduction
in memory footprint and bandwidth requirements. This efficiency gain is
particularly crucial for edge devices and mobile applications where power and
memory constraints are significant limiting factors. Additionally, many
domain-specific neural network accelerators can achieve higher throughput with
fixed-point or reduced-precision floating-point arithmetic, as these simpler
operations allow for more processing units within the same silicon area and
power envelope.

Beyond hardware efficiency, quantization addresses a fundamental question about
neural network representations: how much numerical precision is actually
necessary for effective inference? Neural networks are known to be robust to
various forms of noise and perturbation during training, suggesting they might
not require the full precision of 32-bit floating-point numbers to maintain
their functional capabilities. Understanding this precision requirement is
crucial for both software optimization and hardware design, as it directly
influences decisions about arithmetic unit complexity, memory hierarchy
organization, and data movement strategies in domain-specific accelerators.

This work presents a comprehensive study examining the relationship between
quantization bit depth and model performance across diverse neural network
architectures. Through implementation of a custom quantization framework that
enables exploration of arbitrary bit depths, the study evaluates four distinct
neural architectures: a simple dense network, two different MNIST classifiers
(dense and convolutional), and a transformer-based sequence model.

In summary, my work contributes the following:
\begin{enumerate}
\item A flexible quantization framework for exploring arbitrary bit depths
\item Empirical evaluation of quantization effects across multiple network architectures
\item Analysis of the relationship between bit depth and model performance, introducing a ``signal-to-complexity'' metric
\item Practical insights for informing both software quantization strategies and hardware design decisions
\end{enumerate}

\section{Related Work}
The 1990's paper by \citeauthor{hammerstrom1990digital}
\cite{hammerstrom1990digital} laid the foundation for modern quantization
methods. Hammerstrom demonstrated that neural networks could operate effectively
with simple digital arithmetic, including significantly reduced precision
weights and activations. This early work in VLSI neural implementations
established the fundamental viability of quantized neural computation, though
the neural networks of that era were far simpler than today's deep
architectures.

The modern era of reduced precision neural networks, however, is defined by a
seminal work by \citeauthor{gupta2015deep} \cite{gupta2015deep}, which
demonstrated that deep neural networks could be effectively trained using 16-bit
fixed-point arithmetic when employing stochastic rounding, achieving comparable
performance to 32-bit floating-point computations. The work includes the
implementation of an energy-efficient hardware accelerator that leverages
low-precision fixed-point arithmetic with stochastic rounding.

Building on this seminal work, the 2016 \citeauthor{hubara2016quantized} paper
\cite{hubara2016quantized} introduces Quantized Neural Networks (QNNs), which
constrain neural network weights and activations to low-precision values during
both training and inference. The authors propose a training method that
maintains these quantized values throughout the entire training process while
keeping a high-precision version of the weights for gradient-based updates. The
paper shows that QNNs can achieve near state-of-the-art results on several
datasets including ImageNet.

Google's 2017 work \cite{jouppi2017datacenter} prominently displayed
quantization as a core design element of their Tensor Processing Unit.  The
TPU's successful deployment of 8-bit quantization for large-scale production
workloads provided concrete evidence that reduced precision arithmetic could
deliver substantial performance and energy efficiency improvements in real-world
applications. This hardware implementation helped establish 8-bit quantization
as a de facto standard in many neural network accelerators.

In 2018, NVIDIA's work on mixed precision approaches
\cite{micikevicius2018mixed} showed that different parts of a neural network
could operate at different numerical precisions without significant accuracy
loss. Their work demonstrated that maintaining higher precision in critical
operations while reducing precision elsewhere could optimize the
accuracy-efficiency trade-off. This is invaluable for DSAs, wherein different
computational units might support different levels of precision.

The current state of quantization research was thoroughly summarized by
\cite{gholami2021survey}, who reviewed various quantization methods for
efficient neural network inference. Their work highlighted the diversity of
approaches to quantization and the need for systematic evaluation of these
methods across different neural architectures. 

To summarize, although the prior work thoroughly addresses different
quantization approaches and implementations, the study in
\cite{gholami2021survey} clearly illustrates that two questions remain unanswered:
\begin{enumerate}
    \item What level of precision should a deep learning practitioner quantize
    to for the purposes of balancing efficiency and performance?
    \item How do different neural network architectures compare in their
    response to quantization?
\end{enumerate}

My study will attempt to answer both these questions, and in doing so, hopefully
lay a foundation for implementing more efficient quantized hardware accelerators
for neural networks.

\printbibliography

\end{document}