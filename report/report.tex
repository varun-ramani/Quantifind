\documentclass[twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[style=numeric]{biblatex}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage[margin=0.7in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}

% Bibliography file
\addbibresource{references.bib}

\title{Quantifind: Finding Optimal Quantization Strategies For Diverse Neural Networks}
\author{Varun Ramani}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Your abstract goes here.
\end{abstract}

\section{Introduction}
As deep neural networks continue to dominate the landscape of machine learning
applications, the design of domain-specific hardware accelerators has become
increasingly critical for efficient deployment in deep learning situations.
While these specialized architectures offer significant performance improvements
over general-purpose processors, their design space is heavily influenced by
fundamental assumptions about numerical precision and data representation. One
such assumption -- the bit width used for neural network operations -- can
dramatically impact both the hardware complexity and the achievable performance
of these accelerators.

Model quantization, the process of reducing numerical precision of network
parameters and computations, represents a crucial bridge between neural network
algorithms and their hardware implementations. While current domain-specific
architectures often implement fixed bit widths (typically 8-bit integers or
16-bit floating point numbers) based on conventional wisdom or hardware
constraints, there has been limited systematic investigation into how different
neural architectures respond to varying levels of quantization precision. This
gap in understanding affects not only software optimization but also hardware
design decisions, as the choice of supported bit widths directly impacts circuit
complexity, memory requirements, and energy efficiency of neural network
accelerators.

The motivation for quantization stems from both practical and theoretical
considerations in neural network deployment. From a hardware perspective,
reduced precision operations offer substantial benefits: lower memory bandwidth
requirements, decreased power consumption, and simpler arithmetic units that can
be parallelized more effectively. For example, replacing 32-bit floating-point
operations with 8-bit integer operations can theoretically yield a 4x reduction
in memory footprint and bandwidth requirements. This efficiency gain is
particularly crucial for edge devices and mobile applications where power and
memory constraints are significant limiting factors. Additionally, many
domain-specific neural network accelerators can achieve higher throughput with
fixed-point or reduced-precision floating-point arithmetic, as these simpler
operations allow for more processing units within the same silicon area and
power envelope.

Beyond hardware efficiency, quantization addresses a fundamental question about
neural network representations: how much numerical precision is actually
necessary for effective inference? Neural networks are known to be robust to
various forms of noise and perturbation during training, suggesting they might
not require the full precision of 32-bit floating-point numbers to maintain
their functional capabilities. Understanding this precision requirement is
crucial for both software optimization and hardware design, as it directly
influences decisions about arithmetic unit complexity, memory hierarchy
organization, and data movement strategies in domain-specific accelerators.

This work presents a comprehensive study examining the relationship between
quantization bit depth and model performance across diverse neural network
architectures. Through implementation of a custom quantization framework that
enables exploration of arbitrary bit depths, the study evaluates four distinct
neural architectures: a simple dense network, two different MNIST classifiers
(dense and convolutional), and a transformer-based sequence model.

In summary, my work contributes the following:
\begin{enumerate}
\item A flexible quantization framework for exploring arbitrary bit depths
\item Empirical evaluation of quantization effects across multiple network architectures
\item Analysis of the relationship between bit depth and model performance, introducing a ``signal-to-complexity'' metric
\item Practical insights for informing both software quantization strategies and hardware design decisions
\end{enumerate}

\section{Related Work}
The 1990's paper by \citeauthor{hammerstrom1990digital}
\cite{hammerstrom1990digital} laid the foundation for modern quantization
methods. Hammerstrom demonstrated that neural networks could operate effectively
with simple digital arithmetic, including significantly reduced precision
weights and activations. This early work in VLSI neural implementations
established the fundamental viability of quantized neural computation, though
the neural networks of that era were far simpler than today's deep
architectures.

The modern era of reduced precision neural networks, however, is defined by a
seminal work by \citeauthor{gupta2015deep} \cite{gupta2015deep}, which
demonstrated that deep neural networks could be effectively trained using 16-bit
fixed-point arithmetic when employing stochastic rounding, achieving comparable
performance to 32-bit floating-point computations. The work includes the
implementation of an energy-efficient hardware accelerator that leverages
low-precision fixed-point arithmetic with stochastic rounding.

Building on this seminal work, the 2016 \citeauthor{hubara2016quantized} paper
\cite{hubara2016quantized} introduces Quantized Neural Networks (QNNs), which
constrain neural network weights and activations to low-precision values during
both training and inference. The authors propose a training method that
maintains these quantized values throughout the entire training process while
keeping a high-precision version of the weights for gradient-based updates. The
paper shows that QNNs can achieve near state-of-the-art results on several
datasets including ImageNet.

Google's 2017 work \cite{jouppi2017datacenter} prominently displayed
quantization as a core design element of their Tensor Processing Unit.  The
TPU's successful deployment of 8-bit quantization for large-scale production
workloads provided concrete evidence that reduced precision arithmetic could
deliver substantial performance and energy efficiency improvements in real-world
applications. This hardware implementation helped establish 8-bit quantization
as a de facto standard in many neural network accelerators.

In 2018, NVIDIA's work on mixed precision approaches
\cite{micikevicius2018mixed} showed that different parts of a neural network
could operate at different numerical precisions without significant accuracy
loss. Their work demonstrated that maintaining higher precision in critical
operations while reducing precision elsewhere could optimize the
accuracy-efficiency trade-off. This is invaluable for DSAs, wherein different
computational units might support different levels of precision.

The current state of quantization research was thoroughly summarized by
\cite{gholami2021survey}, who reviewed various quantization methods for
efficient neural network inference. Their work highlighted the diversity of
approaches to quantization and the need for systematic evaluation of these
methods across different neural architectures. 

To summarize, although the prior work thoroughly addresses different
quantization approaches and implementations, the study in
\cite{gholami2021survey} clearly illustrates that two questions remain unanswered:
\begin{enumerate}
    \item What level of precision should a deep learning practitioner quantize
    to for the purposes of balancing efficiency and performance?
    \item How do different neural network architectures compare in their
    response to quantization?
\end{enumerate}

My study will attempt to answer both these questions, and in doing so, hopefully
lay a foundation for implementing more efficient quantized hardware accelerators
for neural networks.

\section{Motivation}
In this era of truly colossal neural networks, the computational demands of deep
learning are swelling exponentially. To meet this need, a new generation of deep
learning-optimized DSAs has appeared: consider AWS Trainium and (as previously
mentioned) Google's focus on TPUs. Rather than focusing on general purpose
parallel acceleration, as per NVIDIA's strategy with their GPUs, these
accelerators solve the unique challenges encountered during deep learning.

Quantization is a crucially important strategy to allow hardware to keep up with
the ever-increasing computational demands of modern deep learning. Consider that
modern language models can require hundreds of gigabytes just to store their
weights in standard 32-bit floating-point precision. This growth in model size
has created a pressing need for more efficient neural network implementations,
particularly for deployment in resource-constrained environments. While various
compression techniques exist, quantization is uniquely beneficial as it can
optimize both computational efficiency and memory utilization without requiring
architectural changes to the neural network itself.

That said, there doesn't seem to be much understanding or literature about the
effects of different quantization levels on different neural network
architectures, making it difficult to deploy quantization-powered hardware
accelerators with confidence.  Currently, many of the decisions underpinning
DSAs for deep learning are based on conventional wisdom---such as the common
choice of 8-bit integer arithmetic---rather than comprehensive empirical
evidence. This is problematic because different neural architectures may have
fundamentally different precision requirements. 

Therfore, in this work, I will attempt to bridge this gap. By examining how
different architectures respond to varying levels of quantization, I want to set
the groundwork for quantized accelerators tailored to the unique needs of their
particular target networks.

\section{Methodology}
In this section, I'll provide a detailed overview of the arbitrary-precision
quantization framework and neural network architectures. The arbitrary precision
framework is interesting because arbitrary precision quantization of floats is
inherently not well defined; in this work, I attempt to define it and then
implement it in code.

\subsection{Quantization Framework}
The quantization framework implements a discretization-based approach to
simulate arbitrary-precision fixed-point arithmetic while operating within
PyTorch's floating-point environment. For a target bit depth $b$, one bit is
reserved for sign, leaving $b-1$ bits for magnitude representation. 

This yields
a discrete signed integer space with $2^{b-1}$ levels in each polarity, defined
by:
\begin{equation*}
(level_{min}, level_{max}) = (-2^{b-1}, 2^{b-1} - 1)
\end{equation*}

For a given tensor $\mathbf{x}$, quantization begins by computing scaling
parameters. Let $x_{min}$ and $x_{max}$ be the minimum and maximum values in
$\mathbf{x}$. To ensure symmetric quantization around zero, these bounds are
adjusted:
\begin{equation*}
x_{max}' = \max(|x_{max}|, |x_{min}|)
\end{equation*}
\begin{equation*}
x_{min}' = -x_{max}'
\end{equation*}
The scaling factor $s$ is then computed to map this range to the discrete integer space:
\begin{equation*}
s = \frac{x_{max}' - x_{min}'}{level_{max} - level_{min}}
\end{equation*}
For signed quantization, the zero point offset is fixed at 0:
\begin{equation*}
z = 0
\end{equation*}
The quantization operation $Q(\mathbf{x})$ proceeds in three steps:

Scale and offset the input values:
\begin{equation*}
\mathbf{x}_{scaled} = \frac{\mathbf{x}}{s} + z
\end{equation*}
Round to nearest integer:
\begin{equation*}
\mathbf{x}_{int} = round(\mathbf{x}_{scaled})
\end{equation*}
Clamp to valid range:
\begin{equation*}
\mathbf{x}_{clamped} = clamp(\mathbf{x}_{int}, level_{min}, level_{max})
\end{equation*}

The final quantized values are obtained by inverting the scaling:
\begin{equation*}
Q(\mathbf{x}) = ({\mathbf{x}_{clamped}} - z) \times s
\end{equation*}
This framework is applied consistently across three contexts:
\begin{enumerate}
    \item Model parameters: Each weight tensor and bias vector in the network is
    independently quantized using its own scaling factors.
    \item Activations: Intermediate tensor outputs are quantized using scaling
    factors computed from their specific value ranges.
    \item Input data: The framework provides a custom Dataset wrapper that
    quantizes input tensors on-the-fly during data loading, maintaining batch
    processing efficiency.
\end{enumerate}
The quantization scheme deliberately avoids true integer arithmetic, instead
using floating-point operations to simulate fixed-point behavior. This design
choice allows for exploration of arbitrary bit depths without requiring
hardware-specific integer implementations, while still accurately modeling the
effects of reduced precision through controlled value discretization.

\subsection{Neural Network Suite}
The study employs four distinct neural architectures to evaluate quantization
effects across different network types and tasks. Each architecture is chosen to
represent different aspects of modern neural network design.

Each network is implemented in PyTorch and trained using the Adam optimizer with
learning rate 0.001. The training process uses full 32-bit precision, with
quantization applied only during evaluation.

\subsubsection{Simple Dense Network}
The simplest architecture consists of a single dense layer with ReLU activation:
\begin{equation*}
f(x) = \text{ReLU}(Wx + b), \quad W \in \mathbb{R}^{256 \times 256}, b \in \mathbb{R}^{256}
\end{equation*}
This network serves as a baseline to understand quantization effects on basic matrix multiplication and activation functions.

\subsubsection{Dense MNIST Classifier}
The dense MNIST classifier implements a three-layer architecture:
\begin{align*}
h_1 &= \text{ReLU}(W_1x + b_1), & W_1 &\in \mathbb{R}^{512 \times 784} \\
h_2 &= \text{ReLU}(W_2h_1 + b_2), & W_2 &\in \mathbb{R}^{512 \times 512} \\
y &= \text{softmax}(W_3h_2 + b_3), & W_3 &\in \mathbb{R}^{10 \times 512}
\end{align*}

\subsubsection{Convolutional MNIST Classifier}
The convolutional architecture introduces spatial operations through three conv-pool blocks:
\begin{align*}
h_1 &= \text{Pool}(\text{ReLU}(\text{Conv}_1(x))), & \text{Conv}_1 &: 1 \rightarrow 32 \text{ channels} \\
h_2 &= \text{Pool}(\text{ReLU}(\text{Conv}_2(h_1))), & \text{Conv}_2 &: 32 \rightarrow 64 \text{ channels} \\
h_3 &= \text{Pool}(\text{ReLU}(\text{Conv}_3(h_2))), & \text{Conv}_3 &: 64 \rightarrow 128 \text{ channels} \\
y &= \text{softmax}(Wh_3 + b), & W &\in \mathbb{R}^{10 \times (128 \times 3 \times 3)}
\end{align*}
where each Conv layer uses $3\times3$ kernels with padding=1, and Pool implements $2\times2$ max pooling.

\subsubsection{Language Transformer}
The transformer architecture processes character-level sequences for language
identification. If a sequence $x$ has length $L$, then the architecture is
computed as follows:
\begin{align*}
E_{char} &= \text{Embedding}(x), \quad E_{char} \in \mathbb{R}^{L \times 512}, \text{ vocab size } = 65535 \nonumber \\
E &= E_{char} + \text{PositionalEmbedding}(L), \quad E \in \mathbb{R}^{L \times 512} \\
H_0 &= E \\
H_1 &= \text{TransformerLayer}_1(H_{0}) \\
... \\
H_{32} &= \text{TransformerLayer}_{32}(H_{31}) \\
H_{mean} &= \text{mean}(H_{32}), \quad H_{mean} \in R^{512} \\
y &= \text{softmax}(\text{Linear}(H_{mean})), \quad y \in \mathbb{R}^{18}
\end{align*}

Transformer layers are used directly from PyTorch with 4 self-attention heads.

\section{Evaluation}
This section details the evaluation methodology used to assess quantization effects across different bit depths and neural architectures.

\subsection{Testing Framework}
The evaluation framework systematically tests each neural network architecture across bit depths ranging from 2 to 32 bits. For a given network and bit depth $b$, the evaluation process consists of:
\begin{enumerate}
\item Quantizing the model parameters using the framework described in Section 3
\item Processing the test dataset through both the original and quantized models
\item Computing comparative metrics between the two versions
\end{enumerate}

\subsection{Evaluation Metrics}
For each bit depth $b$, the framework computes four key metrics:
\begin{align}
\text{Original Loss} &= \mathcal{L}(f_{32}(\mathbf{x}), \mathbf{y}) \nonumber \\
\text{Quantized Loss} &= \mathcal{L}(f_b(\mathbf{x}), \mathbf{y}) \nonumber \\
\text{Signal-to-Complexity Ratio} &= \frac{\text{Quantized Loss}}{b} \nonumber \\
\text{Criterion Ratio} &= \frac{\text{Quantized Loss}}{\text{Original Loss}}
\end{align}
where $f_{32}$ represents the original 32-bit model, $f_b$ represents the $b$-bit quantized model, $\mathbf{x}$ represents the input data, $\mathbf{y}$ represents the target values, and $\mathcal{L}$ represents the appropriate loss function (criterion) for each model:
\begin{itemize}
\item Mean Squared Error loss for the simple dense network
\item Cross Entropy loss for both MNIST classifiers
\item Cross Entropy loss for the language transformer
\end{itemize}
The Signal-to-Complexity Ratio (SCR) is a novel metric introduced in this study
to assess the efficiency of different bit depths. It measures the trade-off
between model performance (quantized loss) and representation complexity (bit
depth). A lower SCR indicates more efficient use of the allocated bits,
suggesting better performance relative to the precision used.

The Criterion Ratio (CR) provides a direct measure of the relative change in the
loss function (criterion) due to quantization, with values closer to 1.0
indicating better preservation of model behavior. This metric enables direct
comparison of how quantization affects the optimization criterion across
different bit depths. By expressing quantized loss as a ratio of the original
loss, it normalizes the metric across different model architectures and loss
functions.

These metrics were chosen to provide complementary views of quantization
effects: while CR measures relative change in the optimization criterion, SCR
provides insight into the efficiency of bit utilization.

\section{Results}
Analysis of quantization effects across the four architectures revealed several
key patterns in how different neural network designs respond to reduced
precision. Please look through the actual result figures, provided in Appendix D.

\subsection{Simple Dense Network}
The simple dense network exhibited a distinct quantization response
characterized by a sharp performance cliff below 4 bits, followed by rapid
stabilization. The criterion ratio shows that with 4-bit quantization, the loss
increased by approximately 10\% compared to the 32-bit baseline. Further
precision reduction to 3 bits resulted in a 20\% loss increase, while 2-bit
quantization led to a 60\% degradation. Notably, performance remained virtually
identical to the baseline for all bit depths above 4 bits, suggesting this
architecture has minimal high-precision requirements.

\subsection{Dense vs. Convolutional MNIST}
Comparing the MNIST classifiers reveals an important architectural effect. The
dense network showed higher sensitivity to extreme quantization, with its
criterion ratio reaching 1.4 at 2 bits compared to 1.2 for the convolutional
network. This suggests that spatial operations in CNNs may provide some inherent
robustness to quantization effects. Moreover, both networks showed minimal
performance impact (less than 1\% loss increase) above 5 bits, indicating that
standard 8-bit quantization provides considerable headroom for these tasks.

\subsection{Language Transformer}
The transformer architecture demonstrated markedly different behavior from the
feed-forward networks. Its criterion ratio exhibited the sharpest initial
degradation, jumping to nearly 1.8 at 3 bits - significantly higher than any
other tested architecture. However, the transformer also showed the fastest
stabilization, with performance nearly matching the baseline above 6 bits. This
suggests that while transformers require higher minimum precision than simpler
architectures, they may not benefit substantially from precisions above 8 bits.

\subsection{Signal-to-Complexity Analysis}
The signal-to-complexity ratio (SCR) revealed optimal bit depths for each
architecture. All networks showed rapidly diminishing returns in SCR above 8
bits, with the steepest efficiency gains occurring between 3-8 bits. The
transformer achieved its optimal SCR at 7 bits, while both MNIST networks peaked
at 6 bits, and the simple dense network at 5 bits. This suggests that commonly
used 8-bit quantization may be unnecessarily precise for simpler architectures.

\subsection{Cross-Architecture Patterns}
Several consistent patterns emerged across all architectures:
\begin{itemize}
\item Performance cliffs consistently appeared below 4 bits, suggesting this may
represent a fundamental lower bound for useful neural computation
\item Minimal benefits were observed above 8 bits, questioning the value of
higher precisions
\item Architecture complexity correlates with minimum precision requirements but
not with optimal bit depth
\item All networks showed remarkable resilience to moderate precision reduction,
maintaining near-baseline performance with 8-bit quantization
\end{itemize}
These findings suggest that hardware accelerator designs should focus on
efficient implementation of 4-8 bit operations, as this range captures the
optimal precision for most architectures while maintaining close to
full-precision performance.

\printbibliography

\appendix
\section{Problems I Faced}
I encountered a few technical challenges worth mentioning during this study:

\subsection{Hardware and Training Issues}
Initial attempts to train the language model proved intractable due to CUDA
memory management and data loading bottlenecks. This was resolved by
restructuring the data loading pipeline to transfer data directly to GPU memory
before use, significantly reducing transfer overhead. However, this solution
required careful memory management to prevent GPU memory exhaustion.
\subsection{Quantization Implementation}
PyTorch's lack of native support for arbitrary-precision float quantization
presented a significant challenge. While PyTorch provides tools for standard
8-bit quantization, exploring the full range of bit depths required a custom
implementation. The discretization strategy implemented in this work, while
somewhat unconventional, provided a practical workaround by simulating
fixed-point arithmetic within floating-point operations.
\subsection{Dataset Quantization}
An unexpected challenge arose in quantizing input data efficiently. The naive
approach of quantizing each batch during training created a significant
computational overhead. This was resolved by implementing a custom Dataset
wrapper that pre-quantizes data while maintaining the ability to modify
quantization parameters between epochs.

\section{Optimizations I Made}
Several key optimizations were implemented to make the evaluation of multiple
quantization levels computationally tractable:
\subsection{GPU Acceleration}
The evaluation framework leverages GPU acceleration throughout the pipeline:
\begin{itemize}
\item Models and data are transferred to GPU memory once and kept there during
evaluation
\item Quantization operations are performed directly on GPU tensors, avoiding
CPU-GPU transfers
\item The custom Dataset wrapper was modified to load data directly to GPU
memory, eliminating transfer overhead during training
\end{itemize}
\subsection{Parallel Evaluation}
The evaluation of different bit depths was parallelized using Python's
ThreadPoolExecutor:
\begin{itemize}
\item Independent worker threads evaluate different bit depths simultaneously
\item A maximum of 4 workers operate concurrently to avoid GPU memory saturation
\item Results are collected asynchronously as they complete
\end{itemize}
\subsection{Caching Strategy}
To avoid redundant computations, the framework implements result caching:
\begin{itemize}
\item Loss values for the 32-bit baseline model are computed once and cached
\item Test results for each bit depth are stored and reused when needed
\item Cache keys are based on model/data combinations to ensure correctness
\end{itemize}
\subsection{Memory Management}
Several optimizations were implemented to manage memory efficiency:
\begin{itemize}
\item Quantized models are created and destroyed sequentially rather than stored
\item Intermediate tensors are explicitly freed after use and result tensors are
explicitly detached from the PyTorch computation graph.
\end{itemize}

\section{GitHub Repository}
The code for this project can be found at \href{https://github.com/varun-ramani/Quantifind}{github.com/varun-ramani/Quantifind}.

\clearpage
\onecolumn

\section{Figures}

\label{appendix:figures}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{vis/simple_dense_vis/crit_ratio.png}
\caption{Criterion ratio across different bit depths for the single-layer dense network, showing the relationship between quantization precision and relative loss increase.}
\label{fig:simple_dense_cr}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{vis/simple_dense_vis/quantized_crit.png}
\caption{Absolute quantized loss values at different bit depths for the single-layer dense network.}
\label{fig:simple_dense_qc}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{vis/simple_dense_vis/sig_to_complex.png}
\caption{Signal-to-complexity ratio across bit depths for the single-layer dense network, measuring efficiency of bit utilization.}
\label{fig:simple_dense_scr}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{vis/dense_mnist_vis/crit_ratio.png}
\caption{Criterion ratio across different bit depths for the three-layer dense MNIST classifier, showing quantization's impact on classification performance.}
\label{fig:dense_mnist_cr}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{vis/dense_mnist_vis/quantized_crit.png}
\caption{Absolute cross-entropy loss values at different bit depths for the dense MNIST classifier.}
\label{fig:dense_mnist_qc}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{vis/dense_mnist_vis/sig_to_complex.png}
\caption{Signal-to-complexity ratio across bit depths for the dense MNIST classifier.}
\label{fig:dense_mnist_scr}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{vis/conv_mnist_vis/crit_ratio.png}
\caption{Criterion ratio across different bit depths for the convolutional MNIST classifier, demonstrating quantization effects on CNN performance.}
\label{fig:conv_mnist_cr}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{vis/conv_mnist_vis/quantized_crit.png}
\caption{Absolute cross-entropy loss values at different bit depths for the convolutional MNIST classifier.}
\label{fig:conv_mnist_qc}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{vis/conv_mnist_vis/sig_to_complex.png}
\caption{Signal-to-complexity ratio across bit depths for the convolutional MNIST classifier.}
\label{fig:conv_mnist_scr}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{vis/language_model_vis/crit_ratio.png}
\caption{Criterion ratio across different bit depths for the language transformer model, showing quantization's impact on language identification performance.}
\label{fig:lang_model_cr}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{vis/language_model_vis/quantized_crit.png}
\caption{Absolute cross-entropy loss values at different bit depths for the language transformer model.}
\label{fig:lang_model_qc}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{vis/language_model_vis/sig_to_complex.png}
\caption{Signal-to-complexity ratio across bit depths for the language transformer model.}
\label{fig:lang_model_scr}
\end{figure}



\end{document}